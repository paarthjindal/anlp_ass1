{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0b832d",
   "metadata": {},
   "source": [
    "# Transformer Model Testing in Google Colab\n",
    "\n",
    "This notebook provides comprehensive testing functionality for your trained Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b1f4d",
   "metadata": {},
   "source": [
    "## 1. Setup and Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbdae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Mount drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Add path for imports\n",
    "sys.path.append('/content/drive/MyDrive/')\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854671fd",
   "metadata": {},
   "source": [
    "## 2. Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bec40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your modules\n",
    "from encoder import TransformerEncoder\n",
    "from decoder import TransformerDecoder, Transformer\n",
    "from utils import (\n",
    "    load_data, split_data, create_vocabulary, TransformerDataset,\n",
    "    create_padding_mask, create_look_ahead_mask, calculate_bleu,\n",
    "    indices_to_sentence, Vocabulary\n",
    ")\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccef8e4",
   "metadata": {},
   "source": [
    "## 3. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bf5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "model_path = '/content/drive/MyDrive/models/best_model.pt'  # Adjust path as needed\n",
    "\n",
    "print(f\"Loading model from {model_path}\")\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Extract configuration and vocabularies\n",
    "src_vocab = checkpoint['src_vocab']\n",
    "tgt_vocab = checkpoint['tgt_vocab']\n",
    "config = checkpoint['args']\n",
    "\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Positional encoding: {config['pos_encoding_type']}\")\n",
    "print(f\"  Model dimension: {config['d_model']}\")\n",
    "print(f\"  Number of heads: {config['num_heads']}\")\n",
    "print(f\"  Encoder layers: {config['num_encoder_layers']}\")\n",
    "print(f\"  Decoder layers: {config['num_decoder_layers']}\")\n",
    "print(f\"  Source vocab size: {len(src_vocab)}\")\n",
    "print(f\"  Target vocab size: {len(tgt_vocab)}\")\n",
    "print(f\"  Training epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae8a67",
   "metadata": {},
   "source": [
    "## 4. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ffa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with same configuration\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    d_model=config['d_model'],\n",
    "    num_heads=config['num_heads'],\n",
    "    num_encoder_layers=config['num_encoder_layers'],\n",
    "    num_decoder_layers=config['num_decoder_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    max_seq_len=config['max_seq_len'],\n",
    "    dropout=config['dropout'],\n",
    "    pos_encoding_type=config['pos_encoding_type']\n",
    ").to(device)\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4b9c4",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9673ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, vocab, max_length=None):\n",
    "    \"\"\"Convert sentence to indices\"\"\"\n",
    "    words = sentence.lower().strip().split()\n",
    "    indices = [vocab.get_idx(word) for word in words]\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = config['max_seq_len']\n",
    "\n",
    "    if len(indices) > max_length - 2:  # -2 for SOS and EOS\n",
    "        indices = indices[:max_length - 2]\n",
    "\n",
    "    # Add SOS and EOS\n",
    "    indices = [vocab.get_idx(vocab.SOS_TOKEN)] + indices + [vocab.get_idx(vocab.EOS_TOKEN)]\n",
    "\n",
    "    # Pad if necessary\n",
    "    while len(indices) < max_length:\n",
    "        indices.append(vocab.get_idx(vocab.PAD_TOKEN))\n",
    "\n",
    "    return indices\n",
    "\n",
    "def translate_greedy(sentence, max_length=50):\n",
    "    \"\"\"Translate a sentence using greedy decoding\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert to indices\n",
    "    src_indices = sentence_to_indices(sentence, src_vocab)\n",
    "    src = torch.tensor([src_indices]).to(device)\n",
    "\n",
    "    # Create source mask\n",
    "    src_mask = create_padding_mask(src, src_vocab.get_idx(src_vocab.PAD_TOKEN))\n",
    "\n",
    "    # Encode source\n",
    "    encoder_output = model.encoder(src, src_mask)\n",
    "\n",
    "    # Start with SOS token\n",
    "    tgt_indices = [tgt_vocab.get_idx(tgt_vocab.SOS_TOKEN)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            tgt = torch.tensor([tgt_indices]).to(device)\n",
    "            tgt_mask = create_look_ahead_mask(tgt, tgt_vocab.get_idx(tgt_vocab.PAD_TOKEN))\n",
    "\n",
    "            # Decode\n",
    "            decoder_output = model.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "            # Get next token\n",
    "            next_token_logits = decoder_output[0, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits).item()\n",
    "\n",
    "            tgt_indices.append(next_token)\n",
    "\n",
    "            # Stop if EOS token\n",
    "            if next_token == tgt_vocab.get_idx(tgt_vocab.EOS_TOKEN):\n",
    "                break\n",
    "\n",
    "    # Convert back to sentence\n",
    "    return indices_to_sentence(tgt_indices, tgt_vocab)\n",
    "\n",
    "def translate_beam_search(sentence, beam_size=4, max_length=50):\n",
    "    \"\"\"Translate a sentence using beam search\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert to indices\n",
    "    src_indices = sentence_to_indices(sentence, src_vocab)\n",
    "    src = torch.tensor([src_indices]).to(device)\n",
    "\n",
    "    # Create source mask\n",
    "    src_mask = create_padding_mask(src, src_vocab.get_idx(src_vocab.PAD_TOKEN))\n",
    "\n",
    "    # Encode source\n",
    "    encoder_output = model.encoder(src, src_mask)\n",
    "\n",
    "    # Initialize beams\n",
    "    beams = [[tgt_vocab.get_idx(tgt_vocab.SOS_TOKEN)]]\n",
    "    beam_scores = [0.0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_length):\n",
    "            candidates = []\n",
    "\n",
    "            for i, beam in enumerate(beams):\n",
    "                if beam[-1] == tgt_vocab.get_idx(tgt_vocab.EOS_TOKEN):\n",
    "                    candidates.append((beam_scores[i], beam))\n",
    "                    continue\n",
    "\n",
    "                tgt = torch.tensor([beam]).to(device)\n",
    "                tgt_mask = create_look_ahead_mask(tgt, tgt_vocab.get_idx(tgt_vocab.PAD_TOKEN))\n",
    "\n",
    "                # Decode\n",
    "                decoder_output = model.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "                # Get probabilities for next token\n",
    "                next_token_logits = decoder_output[0, -1, :]\n",
    "                log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
    "\n",
    "                # Get top-k tokens\n",
    "                top_k_probs, top_k_indices = torch.topk(log_probs, beam_size)\n",
    "\n",
    "                for j in range(beam_size):\n",
    "                    new_beam = beam + [top_k_indices[j].item()]\n",
    "                    new_score = beam_scores[i] + top_k_probs[j].item()\n",
    "                    candidates.append((new_score, new_beam))\n",
    "\n",
    "            # Select top beams\n",
    "            candidates.sort(key=lambda x: x[0], reverse=True)\n",
    "            beams = [cand[1] for cand in candidates[:beam_size]]\n",
    "            beam_scores = [cand[0] for cand in candidates[:beam_size]]\n",
    "\n",
    "            # Check if all beams ended\n",
    "            if all(beam[-1] == tgt_vocab.get_idx(tgt_vocab.EOS_TOKEN) for beam in beams):\n",
    "                break\n",
    "\n",
    "    # Return best beam\n",
    "    best_beam = beams[0]\n",
    "    return indices_to_sentence(best_beam, tgt_vocab)\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b692892",
   "metadata": {},
   "source": [
    "## 6. Test Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d455d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on predefined sample sentences\n",
    "sample_sentences = [\n",
    "    \"Hyvää huomenta\",  # Good morning\n",
    "    \"Kiitos paljon\",   # Thank you very much\n",
    "    \"Nähdään myöhemmin\",  # See you later\n",
    "    \"Mikä on nimesi?\",  # What is your name?\n",
    "    \"Pidän kahvista\",  # I like coffee\n",
    "    \"Sää on kaunis tänään\",  # The weather is beautiful today\n",
    "    \"Voitko auttaa minua?\",  # Can you help me?\n",
    "    \"Olen opiskelija\",  # I am a student\n",
    "    \"Missä on kauppa?\",  # Where is the store?\n",
    "    \"Paljonko tämä maksaa?\"  # How much does this cost?\n",
    "]\n",
    "\n",
    "print(\"Testing Sample Sentences:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, sentence in enumerate(sample_sentences):\n",
    "    print(f\"\\n{i+1}. Finnish: {sentence}\")\n",
    "\n",
    "    try:\n",
    "        greedy_trans = translate_greedy(sentence)\n",
    "        beam_trans = translate_beam_search(sentence, beam_size=4)\n",
    "\n",
    "        print(f\"   Greedy: {greedy_trans}\")\n",
    "        print(f\"   Beam:   {beam_trans}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd348b",
   "metadata": {},
   "source": [
    "## 7. Interactive Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da70fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive translation - run this cell multiple times to test different sentences\n",
    "finnish_sentence = input(\"Enter a Finnish sentence to translate: \")\n",
    "\n",
    "if finnish_sentence.strip():\n",
    "    print(f\"\\nInput: {finnish_sentence}\")\n",
    "\n",
    "    try:\n",
    "        # Greedy translation\n",
    "        greedy_translation = translate_greedy(finnish_sentence)\n",
    "        print(f\"Greedy Translation: {greedy_translation}\")\n",
    "\n",
    "        # Beam search translation\n",
    "        beam_translation = translate_beam_search(finnish_sentence, beam_size=4)\n",
    "        print(f\"Beam Search Translation: {beam_translation}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {e}\")\n",
    "else:\n",
    "    print(\"Please enter a sentence to translate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef1ee7",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17fbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data and evaluate BLEU score\n",
    "print(\"Loading test data...\")\n",
    "\n",
    "# Load original data files\n",
    "data_dir = Path('/content/drive/MyDrive/EUbookshop')\n",
    "src_file = data_dir / 'EUbookshop.fi'\n",
    "tgt_file = data_dir / 'EUbookshop.en'\n",
    "\n",
    "if src_file.exists() and tgt_file.exists():\n",
    "    # Load and split data (same as training)\n",
    "    en_sentences, fi_sentences = load_data(str(tgt_file), str(src_file))\n",
    "    _, _, test_data = split_data(en_sentences, fi_sentences, train_ratio=0.8, val_ratio=0.1)\n",
    "\n",
    "    # Take a subset for evaluation (adjust size as needed)\n",
    "    test_size = 50\n",
    "    test_src = test_data[1][:test_size]  # Finnish sentences\n",
    "    test_tgt = test_data[0][:test_size]  # English sentences\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    print(f\"Generating translations for {test_size} test sentences...\")\n",
    "    for i, src_sentence in enumerate(tqdm(test_src)):\n",
    "        prediction = translate_greedy(src_sentence)\n",
    "        predictions.append(prediction)\n",
    "        references.append(test_tgt[i])\n",
    "\n",
    "        if i < 5:  # Show first 5 examples\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Source (FI): {src_sentence}\")\n",
    "            print(f\"Reference (EN): {test_tgt[i]}\")\n",
    "            print(f\"Prediction (EN): {prediction}\")\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    try:\n",
    "        bleu_score = calculate_bleu(predictions, references)\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"BLEU Score: {bleu_score:.2f}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BLEU score: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Test data files not found. Please ensure the data files are in the correct location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f14aaa",
   "metadata": {},
   "source": [
    "## 9. Model Information Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb364ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive model information\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Model Architecture: Transformer\")\n",
    "print(f\"Positional Encoding: {config['pos_encoding_type']}\")\n",
    "print(f\"Model Dimension: {config['d_model']}\")\n",
    "print(f\"Attention Heads: {config['num_heads']}\")\n",
    "print(f\"Encoder Layers: {config['num_encoder_layers']}\")\n",
    "print(f\"Decoder Layers: {config['num_decoder_layers']}\")\n",
    "print(f\"Feed Forward Dimension: {config['d_ff']}\")\n",
    "print(f\"Max Sequence Length: {config['max_seq_len']}\")\n",
    "print(f\"Dropout Rate: {config['dropout']}\")\n",
    "print(f\"\")\n",
    "print(f\"Vocabulary Sizes:\")\n",
    "print(f\"  Source (Finnish): {len(src_vocab):,}\")\n",
    "print(f\"  Target (English): {len(tgt_vocab):,}\")\n",
    "print(f\"\")\n",
    "print(f\"Training Information:\")\n",
    "print(f\"  Epochs Trained: {checkpoint['epoch']}\")\n",
    "print(f\"  Final Train Loss: {checkpoint['train_loss']:.4f}\")\n",
    "print(f\"  Final Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"  Batch Size: {config['batch_size']}\")\n",
    "print(f\"  Learning Rate: {config['learning_rate']}\")\n",
    "print(f\"\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
