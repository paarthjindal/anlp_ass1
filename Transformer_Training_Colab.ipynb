{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5e7aa3",
   "metadata": {},
   "source": [
    "# Transformer Machine Translation Training in Google Colab\n",
    "\n",
    "This notebook trains your Transformer model with RoPE, Relative Bias, and Sinusoidal positional encodings using Google Colab GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01920f06",
   "metadata": {},
   "source": [
    "## 1. Setup and Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b773a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Check if files exist\n",
    "project_path = '/content/drive/MyDrive/anlp_ass1'  # Adjust this path to your project location\n",
    "print(f\"Project path exists: {os.path.exists(project_path)}\")\n",
    "print(f\"Contents: {os.listdir('/content/drive/MyDrive') if os.path.exists('/content/drive/MyDrive') else 'Drive not mounted'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382fec3",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies and Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install tqdm numpy matplotlib seaborn nltk sacrebleu\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81056cc",
   "metadata": {},
   "source": [
    "## 3. Change to Project Directory and Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_path = '/content/drive/MyDrive/anlp_ass1'  # Adjust this path\n",
    "os.chdir(project_path)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Files in directory: {os.listdir('.')}\")\n",
    "\n",
    "# Import your modules\n",
    "from encoder import TransformerEncoder\n",
    "from decoder import TransformerDecoder, Transformer\n",
    "from utils import (\n",
    "    load_data, split_data, create_vocabulary, TransformerDataset,\n",
    "    create_padding_mask, create_look_ahead_mask, calculate_bleu,\n",
    "    indices_to_sentence, LabelSmoothingLoss\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8874c",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'data_dir': 'EUbookshop',\n",
    "    'src_file': 'EUbookshop.fi',\n",
    "    'tgt_file': 'EUbookshop.en',\n",
    "    'max_seq_len': 128,  # Reduced for Colab GPU memory\n",
    "    'batch_size': 16,    # Reduced for Colab GPU memory\n",
    "    'd_model': 256,      # Reduced for Colab GPU memory\n",
    "    'num_heads': 8,\n",
    "    'num_encoder_layers': 4,  # Reduced for memory\n",
    "    'num_decoder_layers': 4,  # Reduced for memory\n",
    "    'd_ff': 1024,        # Reduced for memory\n",
    "    'dropout': 0.1,\n",
    "    'learning_rate': 0.0001,\n",
    "    'num_epochs': 10,\n",
    "    'pos_encoding_type': 'rope',  # Change to 'relative_bias' or 'sinusoidal' as needed\n",
    "    'warmup_steps': 4000,\n",
    "    'label_smoothing': 0.1,\n",
    "    'clip_grad_norm': 1.0,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "src_file_path = os.path.join(config['data_dir'], config['src_file'])\n",
    "tgt_file_path = os.path.join(config['data_dir'], config['tgt_file'])\n",
    "\n",
    "data_pairs = load_data(src_file_path, tgt_file_path)\n",
    "print(f\"Loaded {len(data_pairs)} sentence pairs\")\n",
    "\n",
    "# Split data\n",
    "train_data, val_data, test_data = split_data(data_pairs, train_ratio=0.8, val_ratio=0.1)\n",
    "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Create vocabularies\n",
    "print(\"Creating vocabularies...\")\n",
    "src_vocab = create_vocabulary([pair[0] for pair in train_data])\n",
    "tgt_vocab = create_vocabulary([pair[1] for pair in train_data])\n",
    "\n",
    "print(f\"Source vocabulary size: {len(src_vocab)}\")\n",
    "print(f\"Target vocabulary size: {len(tgt_vocab)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TransformerDataset(train_data, src_vocab, tgt_vocab, config['max_seq_len'])\n",
    "val_dataset = TransformerDataset(val_data, src_vocab, tgt_vocab, config['max_seq_len'])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571d2ea",
   "metadata": {},
   "source": [
    "## 5. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498e794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(f\"Creating model with {config['pos_encoding_type']} positional encoding...\")\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    d_model=config['d_model'],\n",
    "    num_heads=config['num_heads'],\n",
    "    num_encoder_layers=config['num_encoder_layers'],\n",
    "    num_decoder_layers=config['num_decoder_layers'],\n",
    "    d_ff=config['d_ff'],\n",
    "    max_seq_len=config['max_seq_len'],\n",
    "    dropout=config['dropout'],\n",
    "    pos_encoding_type=config['pos_encoding_type']\n",
    ").to(config['device'])\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: min(\n",
    "        (step + 1) ** -0.5,\n",
    "        (step + 1) * config['warmup_steps'] ** -1.5\n",
    "    )\n",
    ")\n",
    "\n",
    "criterion = LabelSmoothingLoss(\n",
    "    num_classes=len(tgt_vocab),\n",
    "    smoothing=config['label_smoothing'],\n",
    "    ignore_index=tgt_vocab['<pad>']\n",
    ")\n",
    "\n",
    "print(\"Model and training setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb1e8b",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        src, tgt_input, tgt_output = [b.to(config['device']) for b in batch]\n",
    "\n",
    "        # Create masks\n",
    "        src_mask = create_padding_mask(src, src_vocab['<pad>'])\n",
    "        tgt_mask = create_look_ahead_mask(tgt_input, tgt_vocab['<pad>'])\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt_output)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config['clip_grad_norm'])\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        if step % 100 == 0:\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'lr': f'{current_lr:.2e}'\n",
    "            })\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def validate():\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            src, tgt_input, tgt_output = [b.to(config['device']) for b in batch]\n",
    "\n",
    "            # Create masks\n",
    "            src_mask = create_padding_mask(src, src_vocab['<pad>'])\n",
    "            tgt_mask = create_look_ahead_mask(tgt_input, tgt_vocab['<pad>'])\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(src, tgt_input, src_mask, tgt_mask)\n",
    "            loss = criterion(output, tgt_output)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def save_checkpoint(epoch, train_loss, val_loss):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'config': config,\n",
    "        'src_vocab': src_vocab,\n",
    "        'tgt_vocab': tgt_vocab\n",
    "    }\n",
    "\n",
    "    # Create models directory\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    checkpoint_path = f'models/checkpoint_epoch_{epoch}.pt'\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    # Save best model\n",
    "    if not hasattr(save_checkpoint, 'best_val_loss') or val_loss < save_checkpoint.best_val_loss:\n",
    "        save_checkpoint.best_val_loss = val_loss\n",
    "        best_model_path = 'models/best_model.pt'\n",
    "        torch.save(checkpoint, best_model_path)\n",
    "        print(f\"Best model saved: {best_model_path}\")\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca608dea",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(f\"Starting training with {config['pos_encoding_type']} positional encoding...\")\n",
    "print(f\"Total epochs: {config['num_epochs']}\")\n",
    "print(f\"Batch size: {config['batch_size']}\")\n",
    "print(f\"Model dimension: {config['d_model']}\")\n",
    "print(f\"Device: {config['device']}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, config['num_epochs'] + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{config['num_epochs']}\")\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch()\n",
    "\n",
    "    # Validate\n",
    "    val_loss = validate()\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "    save_checkpoint(epoch, train_loss, val_loss)\n",
    "\n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd40cfab",
   "metadata": {},
   "source": [
    "## 8. Test Different Positional Encodings (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d56d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different positional encodings\n",
    "pos_encodings = ['rope', 'relative_bias', 'sinusoidal']\n",
    "results = {}\n",
    "\n",
    "for pos_encoding in pos_encodings:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {pos_encoding} positional encoding\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Update config\n",
    "    config['pos_encoding_type'] = pos_encoding\n",
    "    config['num_epochs'] = 3  # Reduced for comparison\n",
    "\n",
    "    # Create new model\n",
    "    model = Transformer(\n",
    "        src_vocab_size=len(src_vocab),\n",
    "        tgt_vocab_size=len(tgt_vocab),\n",
    "        d_model=config['d_model'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_encoder_layers=config['num_encoder_layers'],\n",
    "        num_decoder_layers=config['num_decoder_layers'],\n",
    "        d_ff=config['d_ff'],\n",
    "        max_seq_len=config['max_seq_len'],\n",
    "        dropout=config['dropout'],\n",
    "        pos_encoding_type=config['pos_encoding_type']\n",
    "    ).to(config['device'])\n",
    "\n",
    "    # Setup optimizer and scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min((step + 1) ** -0.5, (step + 1) * config['warmup_steps'] ** -1.5)\n",
    "    )\n",
    "\n",
    "    # Train for a few epochs\n",
    "    epoch_losses = []\n",
    "    for epoch in range(1, config['num_epochs'] + 1):\n",
    "        train_loss = train_epoch()\n",
    "        val_loss = validate()\n",
    "        epoch_losses.append((train_loss, val_loss))\n",
    "        print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    results[pos_encoding] = epoch_losses\n",
    "\n",
    "    # Save final model\n",
    "    save_checkpoint(f\"{pos_encoding}_final\", train_loss, val_loss)\n",
    "\n",
    "# Print comparison\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "for pos_encoding, losses in results.items():\n",
    "    final_train_loss, final_val_loss = losses[-1]\n",
    "    print(f\"{pos_encoding:15}: Final Train Loss: {final_train_loss:.4f}, Final Val Loss: {final_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ebbba8",
   "metadata": {},
   "source": [
    "## 9. Load and Test Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('models/best_model.pt', map_location=config['device'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Test translation function\n",
    "def translate_sentence(sentence, max_length=50):\n",
    "    \"\"\"Translate a sentence using the trained model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert to indices\n",
    "    tokens = sentence.lower().split()\n",
    "    src_indices = [src_vocab.get(token, src_vocab['<unk>']) for token in tokens]\n",
    "    src_indices = [src_vocab['<start>']] + src_indices + [src_vocab['<end>']]\n",
    "\n",
    "    # Convert to tensor\n",
    "    src = torch.tensor([src_indices]).to(config['device'])\n",
    "    src_mask = create_padding_mask(src, src_vocab['<pad>'])\n",
    "\n",
    "    # Start with <start> token\n",
    "    tgt_indices = [tgt_vocab['<start>']]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        tgt = torch.tensor([tgt_indices]).to(config['device'])\n",
    "        tgt_mask = create_look_ahead_mask(tgt, tgt_vocab['<pad>'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            next_token = output[0, -1].argmax().item()\n",
    "\n",
    "        tgt_indices.append(next_token)\n",
    "\n",
    "        if next_token == tgt_vocab['<end>']:\n",
    "            break\n",
    "\n",
    "    # Convert back to sentence\n",
    "    tgt_tokens = [list(tgt_vocab.keys())[list(tgt_vocab.values()).index(idx)]\n",
    "                  for idx in tgt_indices[1:-1]]  # Skip <start> and <end>\n",
    "\n",
    "    return ' '.join(tgt_tokens)\n",
    "\n",
    "# Test some translations\n",
    "test_sentences = [\n",
    "    \"Hyvää huomenta\",  # Good morning\n",
    "    \"Kiitos paljon\",   # Thank you very much\n",
    "    \"Nähdään myöhemmin\"  # See you later\n",
    "]\n",
    "\n",
    "print(\"\\nTest Translations:\")\n",
    "for sentence in test_sentences:\n",
    "    translation = translate_sentence(sentence)\n",
    "    print(f\"Finnish: {sentence}\")\n",
    "    print(f\"English: {translation}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
